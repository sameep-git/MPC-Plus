"""
Uploader Module
----------------
This module defines the `Uploader` class, responsible for uploading beam data
from model objects to a database. It follows the Program-to-an-Interface principle,
allowing easy switching between different database management systems.

The module includes:
    - DatabaseAdapter: Abstract interface for database operations
    - SupabaseAdapter: Concrete implementation for Supabase DBMS
    - Uploader: Main class that uses model getters to upload data

Supported beam models:
    - Electron beams: `EBeamModel`
    - X-ray beams: `XBeamModel`
    - Geometric beams: `Geo6xfffModel`
"""

from abc import ABC, abstractmethod
from datetime import datetime, date
from decimal import Decimal
from typing import Dict, Any, Optional
import logging
import os
import json

# Set up logger for this module
logger = logging.getLogger(__name__)


class DatabaseAdapter(ABC):
    """
    Abstract interface for database operations.
    Implementations should provide concrete methods for connecting and uploading data.
    """

    @abstractmethod
    def connect(self, connection_params: Dict[str, Any]) -> bool:
        """
        Establish connection to the database.
        
        Args:
            connection_params: Dictionary containing connection parameters
                              (e.g., url, key, etc.)
        
        Returns:
            bool: True if connection successful, False otherwise
        """
        pass

    @abstractmethod
    def upload_beam_data(self, table_name: str, data: Dict[str, Any], path: str = None) -> bool:
        """
        Upload beam data to the specified table.
        
        Args:
            table_name: Name of the database table
            data: Dictionary containing the data to upload
            path: Optional path to extract location from for machine creation
        
        Returns:
            bool: True if upload successful, False otherwise
        """
        pass

    @abstractmethod
    def close(self):
        """Close the database connection."""
        pass


class SupabaseAdapter(DatabaseAdapter):
    """
    Concrete implementation of DatabaseAdapter for Supabase DBMS.
    Uses the supabase-py library to interact with Supabase.
    """

    def __init__(self):
        self.client = None
        self.connected = False

    def connect(self, connection_params: Dict[str, Any]) -> bool:
        """
        Establish connection to Supabase.
        
        Args:
            connection_params: Dictionary with 'url' and 'key' keys
        
        Returns:
            bool: True if connection successful, False otherwise
        """
        try:
            from supabase import create_client, Client
            
            # url = connection_params.get('url')
            # key = connection_params.get('key')
            # print("SUPABASE_URL =", os.getenv("SUPABASE_URL"))
            # print("SUPABASE_URL:", url)

            # connection_params = {
            #     "url": os.getenv("SUPABASE_URL"),
            #     "key": os.getenv("SUPABASE_KEY"),
            # }
            url = connection_params.get('url')
            key = connection_params.get('key')
            # print("SUPABASE_URL =", os.getenv("SUPABASE_URL"))
            # print("SUPABASE_URL:", url)


            if not url or not key:
                logger.error("Supabase connection requires 'url' and 'key' parameters")
                return False
            
            self.client: Client = create_client(url, key)
            self.connected = True
            logger.info("Successfully connected to Supabase")
            return True
            
        except ImportError:
            logger.error("supabase-py library not installed. Install with: pip install supabase")
            return False
        except Exception as e:
            logger.error(f"Error connecting to Supabase: {e}")
            self.connected = False
            return False

    def ensure_machine_exists(self, machine_id: str, path: str = None) -> bool:
        """
        Ensure a machine exists in the machines table before uploading beams.
        Creates the machine if it doesn't exist.
        
        Args:
            machine_id: The machine ID (serial number)
            path: Optional path to extract location from (e.g., "/Volumes/Lexar/MPC Data/Arlington/...")
        
        Returns:
            bool: True if machine exists or was created successfully, False otherwise
        """
        if not self.connected or not self.client:
            logger.error("Not connected to Supabase")
            return False
        
        try:
            # Check if machine exists
            response = self.client.table('machines').select('id').eq('id', machine_id).execute()
            
            if response.data and len(response.data) > 0:
                logger.debug(f"Machine {machine_id} already exists")
                return True
            
            # Machine doesn't exist, create it
            logger.info(f"Creating machine {machine_id}...")
            
            # Extract location from path if provided
            location = "Unknown"
            if path:
                # Try to extract location from path (e.g., "/Volumes/Lexar/MPC Data/Arlington/..." -> "Arlington")
                path_parts = path.split(os.sep)
                for part in path_parts:
                    if part in ["Arlington", "Weatherford"]:
                        location = part
                        break
            
            # Create machine with default values
            machine_data = {
                'id': machine_id,
                'name': f"Machine {machine_id}",
                'location': location,
                'type': 'NDS-WKS'  # Default type based on folder naming pattern
            }
            
            response = self.client.table('machines').insert(machine_data).execute()
            
            if response.data:
                logger.info(f"Created machine {machine_id} in location {location}")
                return True
            else:
                logger.warning(f"No data returned when creating machine {machine_id}")
                return False
                
        except Exception as e:
            logger.error(f"Error ensuring machine exists: {e}", exc_info=True)
            return False

    def upload_beam_data(self, table_name: str, data: Dict[str, Any], path: str = None) -> bool:
        """
        Upload beam data to Supabase table.
        
        Args:
            table_name: Name of the Supabase table
            data: Dictionary containing the data to upload
            path: Optional path to extract location from for machine creation
        
        Returns:
            bool: True if upload successful, False otherwise
        """
        if not self.connected or not self.client:
            logger.error("Not connected to Supabase")
            return False
        
        try:
            # Ensure machine exists before uploading beam
            machine_id = data.get('machineId')
            if machine_id:
                if not self.ensure_machine_exists(machine_id, path):
                    logger.warning(f"Could not ensure machine {machine_id} exists, but continuing with upload attempt")
            
            # Convert Decimal to float for JSON serialization
            serialized_data = self._serialize_data(data)
            logger.debug(f"Uploading data to {table_name}: {serialized_data}")
            
            # Insert data into Supabase table
            response = self.client.table(table_name).insert(serialized_data).execute()
            
            if response.data:
                logger.info(f"Successfully uploaded data to {table_name}")
                return True
            else:
                logger.warning("No data returned from Supabase insert")
                return False
                
        except Exception as e:
            logger.error(f"Error uploading data to Supabase: {e}", exc_info=True)
            return False

    def upload_geocheck_data(self, data: Dict[str, Any], path: str = None) -> Optional[str]:
        """
        Upload geometry check data to geochecks table.
        Note: MLC leaves and backlash should NOT be included here - they go to separate tables.
        
        Args:
            data: Dictionary containing the geocheck data to upload (geometry data only: jaws, couch, gantry, etc.)
            path: Optional path to extract location from for machine creation
        
        Returns:
            str: The geocheck_id if upload successful, None otherwise
        """
        if not self.connected or not self.client:
            logger.error("Not connected to Supabase")
            return None
        
        try:
            # Ensure machine exists before uploading geocheck
            machine_id = data.get('machine_id')
            if machine_id:
                if not self.ensure_machine_exists(machine_id, path):
                    logger.warning(f"Could not ensure machine {machine_id} exists, but continuing with upload attempt")
            
            # Remove MLC data if accidentally included (it goes to separate tables)
            data.pop('mlc_leaves_a', None)
            data.pop('mlc_leaves_b', None)
            data.pop('mlc_backlash_a', None)
            data.pop('mlc_backlash_b', None)
            
            # Convert Decimal to float for JSON serialization
            serialized_data = self._serialize_data(data)
            logger.debug(f"Uploading geocheck data: {serialized_data}")
            
            # Insert data into geochecks table
            response = self.client.table('geochecks').insert(serialized_data).execute()
            
            if response.data and len(response.data) > 0:
                geocheck_id = response.data[0].get('id')
                logger.info(f"Successfully uploaded geocheck data with id: {geocheck_id}")
                return geocheck_id
            else:
                logger.warning("No data returned from Supabase geocheck insert")
                return None
                
        except Exception as e:
            logger.error(f"Error uploading geocheck data to Supabase: {e}", exc_info=True)
            return None

    def upload_mlc_leaves(self, geocheck_id: str, leaves_data: list, bank: str) -> bool:
        """
        Upload MLC leaves data to geocheck_mlc_leaves_a or geocheck_mlc_leaves_b table.
        
        Args:
            geocheck_id: The geocheck ID to associate leaves with
            leaves_data: List of dictionaries with 'leaf_number' and 'leaf_value'
            bank: Either 'a' or 'b' to determine which table to use
        
        Returns:
            bool: True if all leaves uploaded successfully, False otherwise
        """
        if not self.connected or not self.client:
            logger.error("Not connected to Supabase")
            return False
        
        if not geocheck_id or not leaves_data:
            return False
        
        table_name = f'geocheck_mlc_leaves_{bank.lower()}'
        
        try:
            # Prepare data with geocheck_id
            upload_data = []
            for leaf in leaves_data:
                leaf_record = {
                    'geocheck_id': geocheck_id,
                    'leaf_number': leaf.get('leaf_number'),
                    'leaf_value': float(leaf.get('leaf_value')) if leaf.get('leaf_value') is not None else None
                }
                upload_data.append(leaf_record)
            
            # Insert all leaves at once
            response = self.client.table(table_name).insert(upload_data).execute()
            
            if response.data:
                logger.info(f"Successfully uploaded {len(upload_data)} MLC leaves to {table_name}")
                return True
            else:
                logger.warning(f"No data returned from {table_name} insert")
                return False
                
        except Exception as e:
            logger.error(f"Error uploading MLC leaves to {table_name}: {e}", exc_info=True)
            return False

    def upload_mlc_backlash(self, geocheck_id: str, backlash_data: list, bank: str) -> bool:
        """
        Upload MLC backlash data to geocheck_mlc_backlash_a or geocheck_mlc_backlash_b table.
        
        Args:
            geocheck_id: The geocheck ID to associate backlash with
            backlash_data: List of dictionaries with 'leaf_number' and 'backlash_value'
            bank: Either 'a' or 'b' to determine which table to use
        
        Returns:
            bool: True if all backlash data uploaded successfully, False otherwise
        """
        if not self.connected or not self.client:
            logger.error("Not connected to Supabase")
            return False
        
        if not geocheck_id or not backlash_data:
            return False
        
        table_name = f'geocheck_mlc_backlash_{bank.lower()}'
        
        try:
            # Prepare data with geocheck_id
            upload_data = []
            for backlash in backlash_data:
                backlash_record = {
                    'geocheck_id': geocheck_id,
                    'leaf_number': backlash.get('leaf_number'),
                    'backlash_value': float(backlash.get('backlash_value')) if backlash.get('backlash_value') is not None else None
                }
                upload_data.append(backlash_record)
            
            # Insert all backlash records at once
            response = self.client.table(table_name).insert(upload_data).execute()
            
            if response.data:
                logger.info(f"Successfully uploaded {len(upload_data)} MLC backlash records to {table_name}")
                return True
            else:
                logger.warning(f"No data returned from {table_name} insert")
                return False
                
        except Exception as e:
            logger.error(f"Error uploading MLC backlash to {table_name}: {e}", exc_info=True)
            return False

    def _serialize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Convert data types to JSON-serializable formats.
        
        Args:
            data: Dictionary with potentially non-serializable values
        
        Returns:
            Dictionary with serialized values
        """
        serialized = {}
        for key, value in data.items():
            if isinstance(value, Decimal):
                serialized[key] = float(value)
            elif isinstance(value, (datetime, date)):
                # Convert both datetime and date objects to ISO format strings
                serialized[key] = value.isoformat()
            elif value is None:
                serialized[key] = None
            else:
                serialized[key] = value
        return serialized

    def close(self):
        """Close the Supabase connection."""
        self.client = None
        self.connected = False
        logger.info("Supabase connection closed")


class Uploader:
    """
    Handles data upload from model objects to a database.
    Each method corresponds to a specific model type and uses model getters
    to retrieve data for upload.
    """

    def __init__(self, db_adapter: Optional[DatabaseAdapter] = None):
        """
        Initialize the Uploader with a database adapter.
        
        Args:
            db_adapter: Database adapter instance. If None, defaults to SupabaseAdapter
        """
        if db_adapter is None:
            self.db_adapter = SupabaseAdapter()
        else:
            self.db_adapter = db_adapter
        
        self.connected = False

    def connect(self, connection_params: Dict[str, Any]) -> bool:
        """
        Connect to the database using the adapter.
        
        Args:
            connection_params: Dictionary containing connection parameters
        """
        self.connected = self.db_adapter.connect(connection_params)
        return self.connected

    def close(self):
        """
        Close the database connection using the adapter.
        """
        if self.db_adapter:
            self.db_adapter.close()
        self.connected = False

    def upload(self, model):
        """
        Automatically calls the correct upload method
        based on the type of model object passed in.

        Supported models:
            - EBeamModel
            - XBeamModel
            - Geo6xfffModel
        """
        if not self.connected:
            logger.error("Not connected to database. Call connect() first.")
            return False

        model_type = type(model).__name__.lower()
        

        if "ebeam" in model_type:
            return self.eModelUpload(model)
        elif "xbeam" in model_type:
            return self.xModelUpload(model)
        elif "geo" in model_type:
            return self.geoModelUpload(model)
        else:
            raise TypeError(f"Unsupported model type: {type(model).__name__}")

    def _upload_baseline_metrics(self, model, check_type: str):
        """
        Upload baseline data as individual metric records to the baseline table.
        Creates one record per metric (relUniformity, relOutput, centerShift if applicable).
        
        Args:
            model: The beam model (EBeam, XBeam, or GeoModel)
            check_type: "beam" for EBeam/XBeam, "geometry" for GeoModel
        
        Returns:
            bool: True if all metrics uploaded successfully, False otherwise
        """
        try:
            machine_id = model.get_machine_SN()
            beam_variant = model.get_type()  # e.g., "6e", "15x", "6x"
            date = model.get_date()
            
            # List of metrics to upload
            metrics = []
            
            # Add relUniformity
            rel_uniformity = model.get_relative_uniformity()
            if rel_uniformity is not None:
                metrics.append({
                    'machine_id': machine_id,
                    'check_type': check_type,
                    'beam_variant': beam_variant,
                    'metric_type': 'relUniformity',
                    'date': date,
                    'value': rel_uniformity
                })
            
            # Add relOutput
            rel_output = model.get_relative_output()
            if rel_output is not None:
                metrics.append({
                    'machine_id': machine_id,
                    'check_type': check_type,
                    'beam_variant': beam_variant,
                    'metric_type': 'relOutput',
                    'date': date,
                    'value': rel_output
                })
            
            # Add centerShift (only for XBeam and GeoModel, not EBeam)
            if hasattr(model, 'get_center_shift'):
                center_shift = model.get_center_shift()
                if center_shift is not None:
                    metrics.append({
                        'machine_id': machine_id,
                        'check_type': check_type,
                        'beam_variant': beam_variant,
                        'metric_type': 'centerShift',
                        'date': date,
                        'value': center_shift
                    })
            
            # Upload each metric record
            success_count = 0
            for metric_data in metrics:
                if self.db_adapter.upload_beam_data('baselines', metric_data):
                    success_count += 1
                else:
                    print(f"Failed to upload baseline metric: {metric_data['metric_type']}")
            
            print(f"Uploaded {success_count}/{len(metrics)} baseline metric records")
            return success_count == len(metrics) and len(metrics) > 0
            
        except Exception as e:
            print(f"Error uploading baseline metrics: {e}")
            return False

    def uploadTest(self, model):
        """
        Automatically calls the correct upload method
        based on the type of model object passed in, with test output.

        Supported models:
            - EBeamModel
            - XBeamModel
            - Geo6xfffModel
        """
        if not self.connected:
            logger.error("Not connected to database. Call connect() first.")
            return False

        model_type = type(model).__name__.lower()

        if "ebeam" in model_type:
            return self.testeModelUpload(model)
        elif "xbeam" in model_type:
            return self.testxModelUpload(model)
        elif "geo" in model_type:
            return self.testGeoModelUpload(model)
        else:
            raise TypeError(f"Unsupported model type: {type(model).__name__}")

    # --- E-BEAM ---
    def eModelUpload(self, eBeam):
        """
        Upload data for E-beam model to the single beam table or baseline table.
        Maps to schema: type, date, path, relUniformity, relOutput, centerShift, machineId, note
        
        For baselines: Uploads individual metric records to baseline table.
        For regular beams: Uploads single record to beam table.
        """
        try:
            # Check if this is a baseline
            if eBeam.get_baseline():
                # Upload to baseline table as individual metric records
                return self._upload_baseline_metrics(eBeam, check_type='beam')
            else:
                # Prepare data dictionary using model getters, matching the beam table schema
                data = {
                    'type': eBeam.get_type(),
                    'date': eBeam.get_date(),
                    'path': eBeam.get_path(),
                    'relUniformity': eBeam.get_relative_uniformity(),
                    'relOutput': eBeam.get_relative_output(),
                    'centerShift': None,  # E-beams don't have centerShift
                    'machineId': eBeam.get_machine_SN(),
                    'note': None  # Add note if available in the model
                }
                return self.db_adapter.upload_beam_data('beam', data)

        except Exception as e:
            logger.error(f"Error during E-beam upload: {e}", exc_info=True)
            return False


    # --- X-BEAM ---
    def xModelUpload(self, xBeam):
        """
        Upload data for X-beam model to the single beam table or baseline table.
        Maps to schema: type, date, path, relUniformity, relOutput, centerShift, machineId, note
        
        For baselines: Uploads individual metric records to baseline table.
        For regular beams: Uploads single record to beam table.
        """
        try:
            # Check if this is a baseline
            if xBeam.get_baseline():
                # Upload to baseline table as individual metric records
                return self._upload_baseline_metrics(xBeam, check_type='beam')
            else:
                # Prepare data dictionary using model getters, matching the beam table schema
                data = {
                    'type': xBeam.get_type(),
                    'date': xBeam.get_date(),
                    'path': xBeam.get_path(),
                    'relUniformity': xBeam.get_relative_uniformity(),
                    'relOutput': xBeam.get_relative_output(),
                    'centerShift': xBeam.get_center_shift(),
                    'machineId': xBeam.get_machine_SN(),
                    'note': None  # Add note if available in the model
                }
                return self.db_adapter.upload_beam_data('beam', data)

        except Exception as e:
            logger.error(f"Error during X-beam upload: {e}", exc_info=True)
            return False


    # --- GEO MODEL ---
    def geoModelUpload(self, geoModel):
        """
        Upload data for Geo6xfffModel to the single beam table or baseline table.
        Maps to schema: type, date, path, relUniformity, relOutput, centerShift, machineId, note
        
        For baselines: Uploads individual metric records to baseline table.
        For regular beams: Uploads single record to beam table.
        
        Note: Geometry models have additional data (isocenter, gantry, couch, MLC, jaws) 
        that is not stored in the basic beam table. The full extraction code is 
        commented out below for easy re-enabling when geometry tables are created.
        """
        try:
            # Check if this is a baseline
            if geoModel.get_baseline():
                # Upload to baseline table as individual metric records
                return self._upload_baseline_metrics(geoModel, check_type='geometry')
            else:
                # Prepare basic beam data matching the beam table schema
                data = {
                    'type': geoModel.get_type(),
                    'date': geoModel.get_date(),
                    'path': geoModel.get_path(),
                    'relUniformity': geoModel.get_relative_uniformity(),
                    'relOutput': geoModel.get_relative_output(),
                    'centerShift': geoModel.get_center_shift(),
                    'machineId': geoModel.get_machine_SN(),
                    'note': None  # Add note if available in the model
                }
                result = self.db_adapter.upload_beam_data('beam', data)
            
            # ========================================================================
            # COMMENTED OUT: Full geometry data extraction
            # Uncomment when geometry_data table is created
            # ========================================================================
            
            # ---- Extract IsoCenterGroup data ----
            isocenter_data = {
                'beam_id': result_id,  # Foreign key to beam table
                'isoCenterSize': geoModel.get_IsoCenterSize(),
                'isoCenterMVOffset': geoModel.get_IsoCenterMVOffset(),
                'isoCenterKVOffset': geoModel.get_IsoCenterKVOffset(),
            }
            
            # ---- Extract CollimationGroup data ----
            collimation_data = {
                'beam_id': result_id,
                'collimationRotationOffset': geoModel.get_CollimationRotationOffset(),
            }
            
            # ---- Extract GantryGroup data ----
            gantry_data = {
                'beam_id': result_id,
                'gantryAbsolute': geoModel.get_GantryAbsolute(),
                'gantryRelative': geoModel.get_GantryRelative(),
            }
            
            # ---- Extract EnhancedCouchGroup data ----
            couch_data = {
                'beam_id': result_id,
                'couchMaxPositionError': geoModel.get_CouchMaxPositionError(),
                'couchLat': geoModel.get_CouchLat(),
                'couchLng': geoModel.get_CouchLng(),
                'couchVrt': geoModel.get_CouchVrt(),
                'couchRtnFine': geoModel.get_CouchRtnFine(),
                'couchRtnLarge': geoModel.get_CouchRtnLarge(),
                'rotationInducedCouchShiftFullRange': geoModel.get_RotationInducedCouchShiftFullRange(),
            }
            
            # ---- Extract MLC Leaves data (A and B banks, leaves 11-50) ----
            mlc_leaves_a = {}
            mlc_leaves_b = {}
            for i in range(11, 51):
                mlc_leaves_a[f"leaf_{i}"] = geoModel.get_MLCLeafA(i)
                mlc_leaves_b[f"leaf_{i}"] = geoModel.get_MLCLeafB(i)
            
            # ---- Extract MLC Offsets ----
            mlc_offset_data = {
                'beam_id': result_id,
                'mlcMaxOffsetA': geoModel.get_MaxOffsetA(),
                'mlcMaxOffsetB': geoModel.get_MaxOffsetB(),
                'mlcMeanOffsetA': geoModel.get_MeanOffsetA(),
                'mlcMeanOffsetB': geoModel.get_MeanOffsetB(),
                'mlcLeavesA': json.dumps(mlc_leaves_a),  # Store as JSONB
                'mlcLeavesB': json.dumps(mlc_leaves_b),  # Store as JSONB
            }
            
            # ---- Extract MLC Backlash data (A and B banks, leaves 11-50) ----
            mlc_backlash_a = {}
            mlc_backlash_b = {}
            for i in range(11, 51):
                mlc_backlash_a[f"leaf_{i}"] = geoModel.get_MLCBacklashA(i)
                mlc_backlash_b[f"leaf_{i}"] = geoModel.get_MLCBacklashB(i)
            
            mlc_backlash_data = {
                'beam_id': result_id,
                'mlcBacklashMaxA': geoModel.get_MLCBacklashMaxA(),
                'mlcBacklashMaxB': geoModel.get_MLCBacklashMaxB(),
                'mlcBacklashMeanA': geoModel.get_MLCBacklashMeanA(),
                'mlcBacklashMeanB': geoModel.get_MLCBacklashMeanB(),
                'mlcBacklashA': json.dumps(mlc_backlash_a),  # Store as JSONB
                'mlcBacklashB': json.dumps(mlc_backlash_b),  # Store as JSONB
            }
            
            # ---- Extract Jaws data ----
            jaws_data = {
                'beam_id': result_id,
                'jawX1': geoModel.get_JawX1(),
                'jawX2': geoModel.get_JawX2(),
                'jawY1': geoModel.get_JawY1(),
                'jawY2': geoModel.get_JawY2(),
            }
            
            # ---- Extract Jaw Parallelism data ----
            jaw_parallelism_data = {
                'beam_id': result_id,
                'jawParallelismX1': geoModel.get_JawParallelismX1(),
                'jawParallelismX2': geoModel.get_JawParallelismX2(),
                'jawParallelismY1': geoModel.get_JawParallelismY1(),
                'jawParallelismY2': geoModel.get_JawParallelismY2(),
            }
            
            # ---- Upload to geometry tables ----
            # Uncomment and adjust table names when geometry tables are created
            self.db_adapter.upload_beam_data('geometry_isocenter', isocenter_data)
            self.db_adapter.upload_beam_data('geometry_collimation', collimation_data)
            self.db_adapter.upload_beam_data('geometry_gantry', gantry_data)
            self.db_adapter.upload_beam_data('geometry_couch', couch_data)
            self.db_adapter.upload_beam_data('geometry_mlc', mlc_offset_data)
            self.db_adapter.upload_beam_data('geometry_mlc_backlash', mlc_backlash_data)
            self.db_adapter.upload_beam_data('geometry_jaws', jaws_data)
            self.db_adapter.upload_beam_data('geometry_jaw_parallelism', jaw_parallelism_data)
            
            # ========================================================================
            # END OF COMMENTED GEOMETRY DATA
            # ========================================================================
            
            return result

        except Exception as e:
            logger.error(f"Error during Geo model upload: {e}", exc_info=True)
            return False

    def uploadMLCLeaves(self, geoModel, table_name: str = 'mlc_leaves_data'):
        """
        Upload MLC leaf data separately (optional helper method).
        This can be called after geoModelUpload() if you want to store
        individual leaf data in a separate table.
        """
        try:
            leaves_data = []
            
            # Collect all MLC leaf A data (leaves 1-60)
            for i in range(1, 61):
                leaves_data.append({
                    'date': geoModel.get_date(),
                    'machine_sn': geoModel.get_machine_SN(),
                    'leaf_bank': 'A',
                    'leaf_index': i,
                    'leaf_value': geoModel.get_MLCLeafA(i),
                })
            
            # Collect all MLC leaf B data (leaves 1-60)
            for i in range(1, 61):
                leaves_data.append({
                    'date': geoModel.get_date(),
                    'machine_sn': geoModel.get_machine_SN(),
                    'leaf_bank': 'B',
                    'leaf_index': i,
                    'leaf_value': geoModel.get_MLCLeafB(i),
                })
            
            # Upload each leaf record
            success_count = 0
            for leaf_data in leaves_data:
                if self.db_adapter.upload_beam_data(table_name, leaf_data):
                    success_count += 1
            
            logger.info(f"Uploaded {success_count}/{len(leaves_data)} MLC leaf records")
            return success_count == len(leaves_data)

        except Exception as e:
            logger.error(f"Error uploading MLC leaves: {e}", exc_info=True)
            return False

    def uploadMLCBacklash(self, geoModel, table_name: str = 'mlc_backlash_data'):
        """
        Upload MLC backlash data separately (optional helper method).
        This can be called after geoModelUpload() if you want to store
        individual backlash data in a separate table.
        """
        try:
            backlash_data = []
            
            # Collect all MLC backlash A data (leaves 1-60)
            for i in range(1, 61):
                backlash_data.append({
                    'date': geoModel.get_date(),
                    'machine_sn': geoModel.get_machine_SN(),
                    'leaf_bank': 'A',
                    'leaf_index': i,
                    'backlash_value': geoModel.get_MLCBacklashA(i),
                })
            
            # Collect all MLC backlash B data (leaves 1-60)
            for i in range(1, 61):
                backlash_data.append({
                    'date': geoModel.get_date(),
                    'machine_sn': geoModel.get_machine_SN(),
                    'leaf_bank': 'B',
                    'leaf_index': i,
                    'backlash_value': geoModel.get_MLCBacklashB(i),
                })
            
            # Upload each backlash record
            success_count = 0
            for backlash_record in backlash_data:
                if self.db_adapter.upload_beam_data(table_name, backlash_record):
                    success_count += 1
            
            logger.info(f"Uploaded {success_count}/{len(backlash_data)} MLC backlash records")
            return success_count == len(backlash_data)

        except Exception as e:
            logger.error(f"Error uploading MLC backlash: {e}", exc_info=True)
            return False

    def close(self):
        """Close the database connection."""
        if self.db_adapter and hasattr(self.db_adapter, 'close'):
            self.db_adapter.close()


